from flask import Flask, render_template, request, jsonify
import json
import os
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import re
from transformers import pipeline
import torch

app = Flask(__name__)


class ImprovedVATIKAChatbot:
    def __init__(self):
        print("üöÄ Initializing VATIKA Chatbot...")

        # Load multilingual embedding model
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        print("‚úÖ Embedding model loaded")

        # Load QA model with better error handling
        self.qa_pipeline = self.load_qa_model()

        # Initialize data structures
        self.contexts = []
        self.context_embeddings = None
        self.qa_pairs = []  # Store all Q&A pairs separately
        self.qa_embeddings = None

        # Load and process data
        self.load_data()
        print(f"‚úÖ Loaded {len(self.contexts)} contexts and {len(self.qa_pairs)} Q&A pairs")

    def load_qa_model(self):
        """Load QA model with fallback options"""
        models_to_try = [
            "deepset/xlm-roberta-base-squad2",
            "distilbert-base-multilingual-cased",
            "deepset/minilm-uncased-squad2"
        ]

        for model_name in models_to_try:
            try:
                print(f"üîÑ Trying to load QA model: {model_name}")
                qa_pipeline = pipeline(
                    "question-answering",
                    model=model_name,
                    tokenizer=model_name,
                    device=-1  # Use CPU
                )
                print(f"‚úÖ Successfully loaded: {model_name}")
                return qa_pipeline
            except Exception as e:
                print(f"‚ùå Failed to load {model_name}: {e}")
                continue

        print("‚ö†Ô∏è Could not load any QA model, using fallback")
        return None

    def load_data(self):
        """Load and preprocess VATIKA dataset with better error handling"""
        try:
            # Check if data files exist
            train_file = 'data/train.json'
            val_file = 'data/validation.json'

            if not os.path.exists(train_file):
                print("‚ùå Train file not found, creating sample data...")
                self.create_sample_data()

            # Load training data
            with open(train_file, 'r', encoding='utf-8') as f:
                train_data = json.load(f)

            all_data = train_data.get('domains', [])

            # Try to load validation data
            if os.path.exists(val_file):
                try:
                    with open(val_file, 'r', encoding='utf-8') as f:
                        val_data = json.load(f)
                    all_data.extend(val_data.get('domains', []))
                    print("‚úÖ Validation data loaded")
                except Exception as e:
                    print(f"‚ö†Ô∏è Could not load validation data: {e}")

            # Process data
            self.process_data(all_data)

        except Exception as e:
            print(f"‚ùå Error loading data: {e}")
            self.create_fallback_data()

    def process_data(self, domains_data):
        """Process loaded data and create embeddings"""
        all_contexts = []
        all_qas = []

        for domain_data in domains_data:
            domain = domain_data.get('domain', 'unknown')

            for context_data in domain_data.get('contexts', []):
                context_text = context_data.get('context', '')
                qas = context_data.get('qas', [])

                if context_text.strip():  # Only add non-empty contexts
                    context_info = {
                        'domain': domain,
                        'context': context_text,
                        'qas': qas
                    }
                    all_contexts.append(context_info)

                    # Extract Q&A pairs
                    for qa in qas:
                        question = qa.get('question', '').strip()
                        answer = qa.get('answer', '').strip()

                        if question and answer:
                            qa_info = {
                                'question': question,
                                'answer': answer,
                                'domain': domain,
                                'context': context_text
                            }
                            all_qas.append(qa_info)

        self.contexts = all_contexts
        self.qa_pairs = all_qas

        # Create embeddings
        if self.contexts:
            print("üîÑ Creating context embeddings...")
            context_texts = [ctx['context'] for ctx in self.contexts]
            self.context_embeddings = self.embedding_model.encode(context_texts, show_progress_bar=True)

        if self.qa_pairs:
            print("üîÑ Creating Q&A embeddings...")
            qa_questions = [qa['question'] for qa in self.qa_pairs]
            self.qa_embeddings = self.embedding_model.encode(qa_questions, show_progress_bar=True)

    def create_sample_data(self):
        """Create sample data if original data is not available"""
        sample_data = {
            "domains": [
                {
                    "domain": "varanasi_temples",
                    "contexts": [
                        {
                            "context": "‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§î‡§∞ ‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§≠‡§ó‡§µ‡§æ‡§® ‡§∂‡§ø‡§µ ‡§ï‡•ã ‡§∏‡§Æ‡§∞‡•ç‡§™‡§ø‡§§ ‡§π‡•à ‡§î‡§∞ ‡§ó‡§Ç‡§ó‡§æ ‡§®‡§¶‡•Ä ‡§ï‡•á ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä ‡§§‡§ü ‡§™‡§∞ ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à‡•§ ‡§Ø‡§π 12 ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∞‡•ç‡§≤‡§ø‡§Ç‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•á ‡§è‡§ï ‡§π‡•à ‡§î‡§∞ ‡§π‡§ø‡§Ç‡§¶‡•Ç ‡§ß‡§∞‡•ç‡§Æ ‡§Æ‡•á‡§Ç ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§Æ‡§π‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à‡•§",
                            "qas": [
                                {
                                    "question": "‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§ï‡§π‡§æ‡§Å ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à?",
                                    "answer": "‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§ó‡§Ç‡§ó‡§æ ‡§®‡§¶‡•Ä ‡§ï‡•á ‡§™‡§∂‡•ç‡§ö‡§ø‡§Æ‡•Ä ‡§§‡§ü ‡§™‡§∞ ‡§∏‡•ç‡§•‡§ø‡§§ ‡§π‡•à‡•§"
                                },
                                {
                                    "question": "‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§ï‡§ø‡§∏‡•á ‡§∏‡§Æ‡§∞‡•ç‡§™‡§ø‡§§ ‡§π‡•à?",
                                    "answer": "‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§≠‡§ó‡§µ‡§æ‡§® ‡§∂‡§ø‡§µ ‡§ï‡•ã ‡§∏‡§Æ‡§∞‡•ç‡§™‡§ø‡§§ ‡§π‡•à‡•§"
                                },
                                {
                                    "question": "‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∞‡•ç‡§≤‡§ø‡§Ç‡§ó ‡§π‡•à?",
                                    "answer": "‡§π‡§æ‡§Å, ‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞ 12 ‡§ú‡•ç‡§Ø‡•ã‡§§‡§ø‡§∞‡•ç‡§≤‡§ø‡§Ç‡§ó‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∏‡•á ‡§è‡§ï ‡§π‡•à‡•§"
                                }
                            ]
                        }
                    ]
                },
                {
                    "domain": "varanasi_ghats",
                    "contexts": [
                        {
                            "context": "‡§¶‡§∂‡§æ‡§∂‡•ç‡§µ‡§Æ‡•á‡§ß ‡§ò‡§æ‡§ü ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§î‡§∞ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§ò‡§æ‡§ü ‡§π‡•à‡•§ ‡§Ø‡§π‡§æ‡§Å ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡§ø‡§® ‡§∂‡§æ‡§Æ ‡§ï‡•ã ‡§≠‡§µ‡•ç‡§Ø ‡§ó‡§Ç‡§ó‡§æ ‡§Ü‡§∞‡§§‡•Ä ‡§ï‡§æ ‡§Ü‡§Ø‡•ã‡§ú‡§® ‡§π‡•ã‡§§‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π ‡§ò‡§æ‡§ü ‡§Ö‡§§‡•ç‡§Ø‡§Ç‡§§ ‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§Æ‡§æ‡§®‡§æ ‡§ú‡§æ‡§§‡§æ ‡§π‡•à ‡§î‡§∞ ‡§π‡§ú‡§æ‡§∞‡•ã‡§Ç ‡§∂‡•ç‡§∞‡§¶‡•ç‡§ß‡§æ‡§≤‡•Å ‡§î‡§∞ ‡§™‡§∞‡•ç‡§Ø‡§ü‡§ï ‡§Ø‡§π‡§æ‡§Å ‡§Ü‡§§‡•á ‡§π‡•à‡§Ç‡•§",
                            "qas": [
                                {
                                    "question": "‡§¶‡§∂‡§æ‡§∂‡•ç‡§µ‡§Æ‡•á‡§ß ‡§ò‡§æ‡§ü ‡§™‡§∞ ‡§Ü‡§∞‡§§‡•Ä ‡§ï‡§¨ ‡§π‡•ã‡§§‡•Ä ‡§π‡•à?",
                                    "answer": "‡§¶‡§∂‡§æ‡§∂‡•ç‡§µ‡§Æ‡•á‡§ß ‡§ò‡§æ‡§ü ‡§™‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡§ø‡§® ‡§∂‡§æ‡§Æ ‡§ï‡•ã ‡§ó‡§Ç‡§ó‡§æ ‡§Ü‡§∞‡§§‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§"
                                },
                                {
                                    "question": "‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§ò‡§æ‡§ü ‡§ï‡•å‡§® ‡§∏‡§æ ‡§π‡•à?",
                                    "answer": "‡§¶‡§∂‡§æ‡§∂‡•ç‡§µ‡§Æ‡•á‡§ß ‡§ò‡§æ‡§ü ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡§æ ‡§∏‡§¨‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§ò‡§æ‡§ü ‡§π‡•à‡•§"
                                }
                            ]
                        }
                    ]
                }
            ]
        }

        os.makedirs('data', exist_ok=True)
        with open('data/train.json', 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)

        print("‚úÖ Sample data created")

    def create_fallback_data(self):
        """Create minimal fallback data"""
        self.contexts = [{
            'domain': 'general',
            'context': '‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§≠‡§æ‡§∞‡§§ ‡§ï‡§æ ‡§è‡§ï ‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§î‡§∞ ‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§∂‡§π‡§∞ ‡§π‡•à‡•§',
            'qas': [{'question': '‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?', 'answer': '‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§≠‡§æ‡§∞‡§§ ‡§ï‡§æ ‡§è‡§ï ‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§î‡§∞ ‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§∂‡§π‡§∞ ‡§π‡•à‡•§'}]
        }]

        self.qa_pairs = [{'question': '‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?', 'answer': '‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§≠‡§æ‡§∞‡§§ ‡§ï‡§æ ‡§è‡§ï ‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§î‡§∞ ‡§™‡§µ‡§ø‡§§‡•ç‡§∞ ‡§∂‡§π‡§∞ ‡§π‡•à‡•§',
                          'domain': 'general'}]

        context_texts = [ctx['context'] for ctx in self.contexts]
        self.context_embeddings = self.embedding_model.encode(context_texts)

        qa_questions = [qa['question'] for qa in self.qa_pairs]
        self.qa_embeddings = self.embedding_model.encode(qa_questions)

    def find_best_qa_match(self, query, threshold=0.6):
        """Find best matching Q&A pair"""
        if not self.qa_pairs or self.qa_embeddings is None:
            return None

        query_embedding = self.embedding_model.encode([query])
        similarities = cosine_similarity(query_embedding, self.qa_embeddings)[0]

        best_idx = np.argmax(similarities)
        best_score = similarities[best_idx]

        if best_score > threshold:
            return {
                'qa': self.qa_pairs[best_idx],
                'score': best_score
            }

        return None

    def find_relevant_context(self, query, top_k=3, threshold=0.3):
        """Find most relevant contexts"""
        if not self.contexts or self.context_embeddings is None:
            return []

        query_embedding = self.embedding_model.encode([query])
        similarities = cosine_similarity(query_embedding, self.context_embeddings)[0]

        top_indices = np.argsort(similarities)[-top_k:][::-1]

        relevant_contexts = []
        for idx in top_indices:
            if similarities[idx] > threshold:
                relevant_contexts.append({
                    'context': self.contexts[idx],
                    'similarity': similarities[idx]
                })

        return relevant_contexts

    def generate_qa_answer(self, question, context):
        """Generate answer using QA model"""
        if not self.qa_pipeline:
            return None

        try:
            # Truncate context if too long
            max_context_length = 500
            if len(context) > max_context_length:
                context = context[:max_context_length] + "..."

            result = self.qa_pipeline(question=question, context=context)

            if result['score'] > 0.15:  # Confidence threshold
                return result['answer']

        except Exception as e:
            print(f"QA Pipeline error: {e}")

        return None

    def get_smart_fallback(self, query):
        """Generate smart fallback responses"""
        query_lower = query.lower()

        # Keywords-based responses
        responses = {
            ('‡§Æ‡§Ç‡§¶‡§ø‡§∞',
             'temple'): "‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§ï‡§æ‡§∂‡•Ä ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§Ç‡§¶‡§ø‡§∞, ‡§∏‡§Ç‡§ï‡§ü ‡§Æ‡•ã‡§ö‡§® ‡§π‡§®‡•Å‡§Æ‡§æ‡§® ‡§Æ‡§Ç‡§¶‡§ø‡§∞, ‡§¶‡•Å‡§∞‡•ç‡§ó‡§æ ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§ú‡•à‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§π‡•à‡§Ç‡•§ ‡§ï‡§ø‡§∏‡•Ä ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§õ‡•á‡§Ç‡•§",
            ('‡§ò‡§æ‡§ü',
             'ghat'): "‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§¶‡§∂‡§æ‡§∂‡•ç‡§µ‡§Æ‡•á‡§ß ‡§ò‡§æ‡§ü, ‡§Æ‡§£‡§ø‡§ï‡§∞‡•ç‡§£‡§ø‡§ï‡§æ ‡§ò‡§æ‡§ü, ‡§Ö‡§∏‡•ç‡§∏‡•Ä ‡§ò‡§æ‡§ü ‡§ú‡•à‡§∏‡•á ‡§™‡•ç‡§∞‡§∏‡§ø‡§¶‡•ç‡§ß ‡§ò‡§æ‡§ü ‡§π‡•à‡§Ç‡•§ ‡§ï‡§ø‡§∏‡•Ä ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ò‡§æ‡§ü ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§®‡§æ ‡§ö‡§æ‡§π‡§§‡•á ‡§π‡•à‡§Ç?",
            ('‡§Ü‡§∞‡§§‡•Ä', 'aarti'): "‡§ó‡§Ç‡§ó‡§æ ‡§Ü‡§∞‡§§‡•Ä ‡§¶‡§∂‡§æ‡§∂‡•ç‡§µ‡§Æ‡•á‡§ß ‡§ò‡§æ‡§ü ‡§™‡§∞ ‡§™‡•ç‡§∞‡§§‡§ø‡§¶‡§ø‡§® ‡§∂‡§æ‡§Æ ‡§ï‡•ã ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ ‡§Ø‡§π ‡§¨‡§π‡•Å‡§§ ‡§π‡•Ä ‡§Æ‡§®‡•ã‡§π‡§∞ ‡§î‡§∞ ‡§≠‡§µ‡•ç‡§Ø ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§",
            ('‡§ó‡§Ç‡§ó‡§æ', 'ganga'): "‡§ó‡§Ç‡§ó‡§æ ‡§®‡§¶‡•Ä ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡•Ä ‡§ú‡•Ä‡§µ‡§®‡§ß‡§æ‡§∞‡§æ ‡§π‡•à‡•§ ‡§Ø‡§π‡§æ‡§Å ‡§≤‡•ã‡§ó ‡§∏‡•ç‡§®‡§æ‡§® ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç ‡§î‡§∞ ‡§Ü‡§∞‡§§‡•Ä ‡§¶‡•á‡§ñ‡§§‡•á ‡§π‡•à‡§Ç‡•§",
            ('‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ', 'travel',
             '‡§ò‡•Ç‡§Æ‡§®‡§æ'): "‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§Ü‡§™ ‡§Æ‡§Ç‡§¶‡§ø‡§∞, ‡§ò‡§æ‡§ü, ‡§ó‡§≤‡§ø‡§Ø‡§æ‡§Å, ‡§î‡§∞ ‡§∏‡§æ‡§Ç‡§∏‡•ç‡§ï‡•É‡§§‡§ø‡§ï ‡§∏‡•ç‡§•‡§≤ ‡§¶‡•á‡§ñ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§ ‡§ï‡•ç‡§Ø‡§æ ‡§µ‡§ø‡§∂‡§ø‡§∑‡•ç‡§ü ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è?"
        }

        for keywords, response in responses.items():
            if any(keyword in query_lower for keyword in keywords):
                return response

        return "‡§Æ‡•Å‡§ù‡•á ‡§µ‡§æ‡§∞‡§æ‡§£‡§∏‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§Ü‡§™‡§ï‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§∏‡§Æ‡§ù ‡§®‡§π‡•Ä‡§Ç ‡§Ü‡§Ø‡§æ‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Æ‡§Ç‡§¶‡§ø‡§∞, ‡§ò‡§æ‡§ü, ‡§Ü‡§∞‡§§‡•Ä, ‡§Ø‡§æ ‡§Ø‡§æ‡§§‡•ç‡§∞‡§æ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§™‡•Ç‡§õ‡•á‡§Ç‡•§"

    def process_query(self, query):
        """Main query processing function"""
        if not query.strip():
            return "‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ö‡§™‡§®‡§æ ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§™‡•Ç‡§õ‡•á‡§Ç‡•§"

        print(f"üîç Processing query: {query}")

        # Step 1: Try to find direct Q&A match
        qa_match = self.find_best_qa_match(query)
        if qa_match:
            print(f"‚úÖ Found Q&A match with score: {qa_match['score']:.3f}")
            return qa_match['qa']['answer']

        # Step 2: Find relevant contexts
        relevant_contexts = self.find_relevant_context(query)

        if relevant_contexts:
            print(f"‚úÖ Found {len(relevant_contexts)} relevant contexts")

            # Step 3: Try QA model on best context
            best_context = relevant_contexts[0]['context']
            qa_answer = self.generate_qa_answer(query, best_context['context'])

            if qa_answer:
                return qa_answer

            # Step 4: Check for direct Q&As in the context
            for qa in best_context['qas']:
                if self.is_similar_question(query, qa['question']):
                    return qa['answer']

        # Step 5: Smart fallback
        return self.get_smart_fallback(query)

    def is_similar_question(self, q1, q2, threshold=0.7):
        """Check if two questions are similar"""
        try:
            embeddings = self.embedding_model.encode([q1, q2])
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            return similarity > threshold
        except:
            return False


# Initialize improved chatbot
chatbot = ImprovedVATIKAChatbot()


@app.route('/')
def home():
    return render_template('index.html')


@app.route('/chat', methods=['POST'])
def chat():
    try:
        data = request.get_json()
        user_message = data.get('message', '').strip()

        if not user_message:
            return jsonify({'error': '‡§ï‡•É‡§™‡§Ø‡§æ ‡§ï‡•ã‡§à ‡§∏‡§Ç‡§¶‡•á‡§∂ ‡§≠‡•á‡§ú‡•á‡§Ç'}), 400

        # Process the query
        response = chatbot.process_query(user_message)

        # Add debug info in development
        debug_info = {
            'total_contexts': len(chatbot.contexts),
            'total_qas': len(chatbot.qa_pairs),
            'model_loaded': chatbot.qa_pipeline is not None
        }

        return jsonify({
            'response': response,
            'status': 'success',
            'debug': debug_info if app.debug else None
        })

    except Exception as e:
        print(f"‚ùå Chat error: {e}")
        return jsonify({
            'error': f'‡§ï‡•Å‡§õ ‡§ó‡§≤‡§§‡•Ä ‡§π‡•Å‡§à ‡§π‡•à: {str(e)}',
            'status': 'error'
        }), 500


@app.route('/health')
def health():
    return jsonify({
        'status': 'healthy',
        'contexts_loaded': len(chatbot.contexts),
        'qas_loaded': len(chatbot.qa_pairs),
        'embeddings_ready': chatbot.context_embeddings is not None,
        'qa_model_loaded': chatbot.qa_pipeline is not None
    })


@app.route('/debug')
def debug():
    """Debug endpoint to check data"""
    return jsonify({
        'contexts': len(chatbot.contexts),
        'qa_pairs': len(chatbot.qa_pairs),
        'sample_context': chatbot.contexts[0] if chatbot.contexts else None,
        'sample_qa': chatbot.qa_pairs[0] if chatbot.qa_pairs else None
    })


if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)